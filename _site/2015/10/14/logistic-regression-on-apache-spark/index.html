<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  
  <head>
  <meta charset="UTF-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Logistic Regression with Apache Spark &middot; Recipe for Intelligence
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="Recipe for Intelligence" href="/atom.xml">
</head>



  <body>

    <div class="container content">
      <div class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home">Recipe for Intelligence</a>

          
              &nbsp;&nbsp;&nbsp;<small><a href="/about">About</a></small>
          
              &nbsp;&nbsp;&nbsp;<small><a href="/archive">Archive</a></small>
          

        </h3>
      </div>

      <article class="post">
  <h1 class="post-title">Logistic Regression with Apache Spark</h1>
  <time datetime="2015-10-14T00:00:00-07:00" class="post-date">14 Oct 2015</time>
  <p>Logistic regression is widely used to predict binary responses. It is a linear method as described 
in my other post 
<a href="http://loganyc1934.github.io/2015/10/13/an-overview-of-logistic-regression/">An Overview of Logistic Regression</a>.</p>

<p>Binary logistic regression can be generalized into multinomial logistic regression to train and 
predict multiclass classification problems. For example, for K possible outcomes, one of the outcomes 
can be chosen as a “pivot”, and the other K−1 outcomes can be separately regressed against the pivot 
outcome. In MLlib, the first class 0 is chosen as the “pivot” class.</p>

<p>For multiclass classification problems, the algorithm will output a multinomial logistic regression 
model, which contains K−1 binary logistic regression models regressed against the first class. Given 
a new data points, K−1 models will be run, and the class with largest probability will be chosen as 
the predicted class.</p>

<p>Spark implemented two algorithms to solve logistic regression: gradient descent and L-BFGS. Here we
focus on gradient descent.</p>

<p><br></p>

<h4 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h4>

<p>Optimization problems whose objective function f is written as a sum are particularly suitable to be 
solved using stochastic gradient descent (SGD). For logistic regression, the optimization formulation 
commonly used:</p>

<p>$$
f(\mathbf{w})=\frac { 1 }{ n } \sum _{ i=1 }^{ n }{ L(\mathbf{w};{ \mathbf{x} }_{ i },{ y }_{ i }) } 
+\lambda R(\mathbf{w})
$$</p>

<p>where the logistic loss \( L \) is</p>

<p>$$
L(\mathbf{w};\mathbf{x},y)=log(1+exp(-y{ \mathbf{w} }^{ T }\mathbf{x}))
$$</p>

<p>The loss is written as an average of the individual losses coming from each data point.</p>

<p>A stochastic subgradient is a randomized choice of a vector, such that in expectation, we obtain a 
true subgradient of the original objective function. Picking one data point \( i\in [1,...,n] \) 
uniformly at random, we obtain a stochastic subgradient, with respect to \(\mathbf{w} \) as follows:</p>

<p>$$
{ f }&#39;_{ \mathbf{w},i }={ L }&#39;_{ \mathbf{w},i }+\lambda { R }&#39;_{ \mathbf{w},i }
$$</p>

<p>This is determine by the i-th data point. Running SGD simply becomes walking in the direction of the
negative stochastic subgradient \( { f }&#39;_{ w,i } \), that is</p>

<p>$$
{ \mathbf{w} }^{ (t+1) }={ \mathbf{w} }^{ (t) }-\gamma { f }&#39;_{ \mathbf{w},i }
$$</p>

<p><strong>Step-size</strong>. The parameter \( \gamma \) is the step-size, which in the default implementation is 
chosen decreasing with the square root of the iteration counter, i.e. \( \gamma =\frac { s }{ \sqrt { t }  }  \)
in the \(t\)-th iteration, 
with the input parameter s = <code>stepSize</code>. Note that selecting the best step-size 
for SGD methods can often be delicate in practice and is a topic of active research.</p>

<p><strong>Gradients</strong>. Here is a table of (sub)gradients of the machine learning methods implemented in MLlib.</p>

<p><br>
<center><img src="/assets/images/logreg/lossAndGradients.png" alt="Drawing" style="width: 900px;"></center>
<br></p>

<p><br></p>

<h4 id="update-schemes-for-distributed-sgd">Update Schemes for Distributed-SGD</h4>

<p>The SGD implementation in 
<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.optimization.GradientDescent">GradientDescent</a>
uses a simple distributed sampling of the data points. </p>

<p>Recall that the loss part of the optimization problem
is \( \frac { 1 }{ n } \sum _{ i=1 }^{ n }{ L(\mathbf{w};{ \mathbf{x} }_{ i },{ y }_{ i }) } \),
and therefore \( \frac { 1 }{ n } \sum _{ i=1 }^{ n }{ { L }&#39;_{ w,i } } \) would be the true
subgradient. Since this would require the access to the full data set, the parameter <code>miniBatchFraction</code>
specifies which fraction of the full data to use instead.
The average of the gradients over this subset, i.e.</p>

<p>$$
\frac { 1 }{ \left| S \right|  } \sum _{ i\in S }^{  }{ { L }&#39;_{ w,i } } 
$$</p>

<p>is a stochastic gradient. Here \( S \) is the sampled subset of size 
\( \left| S \right|= \) <code>miniBatchFraction</code> \( \cdot n \)</p>

<p>In each iteration, the sampling over the distributed dataset (RDD), as well as the computation of 
the sum of the partial results from each worker machine is performed by the standard spark routines.</p>

<p>If the fraction of points <code>miniBatchFraction</code> is set to 1 (default), then the resulting step in each 
iteration is exact (sub)gradient descent. In this case there is no randomness and no variance in the 
used step directions. </p>

<p>On the other extreme, if miniBatchFraction is chosen very small, such that only 
a single point is sampled, i.e. \( \left| S \right|= \) <code>miniBatchFraction</code> \( \cdot n = 1\), 
then the algorithm is equivalent to standard SGD. 
In that case, the step direction depends from the uniformly random sampling of the point.</p>

<p><br></p>

<h4 id="sgd-implementation-in-mllib">SGD Implementation in MLlib</h4>

<p><em>class</em> pyspark.mllib.classification.<strong>LogisticRegressionWithSGD</strong></p>

<p><em>classmethod</em> <strong>train</strong>(<em>data, iterations=100, step=1.0, miniBatchFraction=1.0, 
initialWeights=None, regParam=0.01, regType=&#39;l2&#39;, intercept=False, validateData=True</em>)</p>

<p>Train a logistic regression model on the given data.</p>

<p>Parameters: </p>

<ul>
<li><code>data</code> – The training data, an RDD of LabeledPoint.</li>
<li><code>iterations</code> – The number of iterations (default: 100).</li>
<li><code>step</code> – The step parameter used in SGD (default: 1.0).</li>
<li><code>miniBatchFraction</code> – Fraction of data to be used for each SGD iteration (default: 1.0).</li>
<li><code>initialWeights</code> – The initial weights (default: None).</li>
<li><code>regParam</code> – The regularizer parameter (default: 0.01).</li>
<li><code>regType</code> – The type of regularizer used for training our model. Allowed values: 

<ul>
<li>“l1” for using L1 regularization</li>
<li>“l2” for using L2 regularization</li>
<li>None for no regularization</li>
<li>(default: “l2”)</li>
</ul></li>
<li>intercept – Boolean parameter which indicates the use or not of the augmented representation for 
training data (i.e. whether bias features are activated or not, default: False).</li>
<li>validateData – Boolean parameter which indicates if the algorithm should validate data before 
training. (default: True)</li>
</ul>

<p><br></p>

<h4 id="click-through-rate-ctr-prediction-with-mllib">Click-Through Rate (CTR) Prediction with MLlib</h4>

<p>Here I will show an example of utilizing 
<a href="https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.classification.LogisticRegressionWithSGD">LogisticRegressionWithSGD</a>
to predict CTR given some training examples. The dataset is provided by Criteo Labs to a Kaggle competition.
It has roughly 51M data points, splitted into 39M for training, 6M for validation and 6M for test. 
Each data point has 233K one-hot-encoded binary features (originally some features had a huge number of
categories before one-hot-encoding).</p>

<p>In MLlib, each labeled data point is represented as the object
<code>LabeledPoint(label, featureVector)</code>. The training set is an RDD of a lot of <code>LabeledPoint</code> objects,
<code>OHETrainData</code>. </p>

<p>First, use <code>LogisticRegressionWithSGD</code> to train a model using <code>OHETrainData</code> with
the given hyperparameter configuration. <code>LogisticRegressionWithSGD</code> returns a
<a href="https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LogisticRegressionModel">LogisticRegressionModel</a>.
Next, use the <code>LogisticRegressionModel.weights</code> and <code>LogisticRegressionModel.intercept</code> attributes to
print out the model&#39;s parameters.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">pyspark.mllib.classification</span> <span class="kn">import</span> <span class="n">LogisticRegressionWithSGD</span>

<span class="c"># fixed hyperparameters</span>
<span class="n">numIters</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">stepSize</span> <span class="o">=</span> <span class="mf">10.</span>
<span class="n">regParam</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">regType</span> <span class="o">=</span> <span class="s">&#39;l2&#39;</span>
<span class="n">includeIntercept</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">model0</span> <span class="o">=</span> <span class="n">LogisticRegressionWithSGD</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">OHETrainData</span><span class="p">,</span> <span class="n">iterations</span><span class="o">=</span><span class="n">numIters</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">stepSize</span><span class="p">,</span> 
                                         <span class="n">regParam</span><span class="o">=</span><span class="n">regParam</span><span class="p">,</span> <span class="n">regType</span><span class="o">=</span><span class="n">regType</span><span class="p">,</span> <span class="n">intercept</span><span class="o">=</span><span class="n">includeIntercept</span><span class="p">)</span>
<span class="n">sortedWeights</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">model0</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>
<span class="k">print</span> <span class="n">sortedWeights</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span> <span class="n">model0</span><span class="o">.</span><span class="n">intercept</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">output:</span>
<span class="sd">[-0.4589923685357562, -0.37973707648623972, -0.3699655826675331, -0.36934962879928285, -0.32697945415010637] 0.56455084025</span>
<span class="sd">&#39;&#39;&#39;</span>
</code></pre></div>
<p>log loss is used to evaluate the model. To compute the log loss, we first need the predictions. To
generate predictions from our trained model, we use the model&#39;s weight and intercept attributes and 
pass them into the sigmoid function.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">exp</span> <span class="c">#  exp(-t) = e^-t</span>

<span class="k">def</span> <span class="nf">getP</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">intercept</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate the probability for an observation given a set of weights and intercept.</span>

<span class="sd">    Note:</span>
<span class="sd">        We&#39;ll bound our raw prediction between 20 and -20 for numerical purposes.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (SparseVector): A vector with values of 1.0 for features that exist in this</span>
<span class="sd">            observation and 0.0 otherwise.</span>
<span class="sd">        w (DenseVector): A vector of weights (betas) for the model.</span>
<span class="sd">        intercept (float): The model&#39;s intercept.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: A probability between 0 and 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rawPrediction</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">intercept</span>

    <span class="c"># Bound the raw prediction value</span>
    <span class="n">rawPrediction</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">rawPrediction</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">rawPrediction</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">rawPrediction</span><span class="p">,</span> <span class="o">-</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">rawPrediction</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pred</span>

<span class="n">trainingPredictions</span> <span class="o">=</span> <span class="n">OHETrainData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">getP</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="n">model0</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span> <span class="n">model0</span><span class="o">.</span><span class="n">intercept</span><span class="p">))</span>

<span class="k">print</span> <span class="n">trainingPredictions</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">output:</span>
<span class="sd">[0.30262882023911125, 0.10362661997434075, 0.283634247838756, 0.17846102057880114, 0.5389775379218853]</span>
<span class="sd">&#39;&#39;&#39;</span>
</code></pre></div>
<p>Now the \( p \)&#39;s are calculated (probabilities for the data points to be in class y = 1), the 
log loss can be computed with \( p \) and the true labels \( y \).</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">log</span>

<span class="k">def</span> <span class="nf">computeLogLoss</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculates the value of log loss for a given probabilty and label.</span>

<span class="sd">    Note:</span>
<span class="sd">        log(0) is undefined, so when p is 0 we need to add a small value (epsilon) to it</span>
<span class="sd">        and when p is 1 we need to subtract a small value (epsilon) from it.</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float): A probabilty between 0 and 1.</span>
<span class="sd">        y (int): A label.  Takes on the values 0 and 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: The log loss value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">10e-12</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">+=</span> <span class="n">epsilon</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">-=</span> <span class="n">epsilon</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>

<span class="n">classOneFracTrain</span> <span class="o">=</span> <span class="n">OHETrainData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span> <span class="n">classOneFracTrain</span>

<span class="n">logLossTrBase</span> <span class="o">=</span> <span class="n">OHETrainData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">computeLogLoss</span><span class="p">(</span><span class="n">classOneFracTrain</span><span class="p">,</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">print</span> <span class="s">&#39;Baseline Train Logloss = {0:.3f}</span><span class="se">\n</span><span class="s">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logLossTrBase</span><span class="p">)</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">output:</span>
<span class="sd">0.22717773523</span>
<span class="sd">Baseline Train Logloss = 0.536</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="k">def</span> <span class="nf">evaluateResults</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculates the log loss for the data given the model.</span>

<span class="sd">    Args:</span>
<span class="sd">        model (LogisticRegressionModel): A trained logistic regression model.</span>
<span class="sd">        data (RDD of LabeledPoint): Labels and features for each observation.</span>

<span class="sd">    Returns:</span>
<span class="sd">        float: Log loss for the data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">weights</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">intercept</span>
    <span class="n">logloss</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">computeLogLoss</span><span class="p">(</span><span class="n">getP</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">features</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">intercept</span><span class="p">),</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">logloss</span>

<span class="n">logLossTrLR0</span> <span class="o">=</span> <span class="n">evaluateResults</span><span class="p">(</span><span class="n">model0</span><span class="p">,</span> <span class="n">OHETrainData</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&#39;OHE Features Train Logloss:</span><span class="se">\n\t</span><span class="s">Baseline = {0:.3f}</span><span class="se">\n\t</span><span class="s">LogReg = {1:.3f}&#39;</span>
       <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logLossTrBase</span><span class="p">,</span> <span class="n">logLossTrLR0</span><span class="p">))</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">output:</span>
<span class="sd">OHE Features Train Logloss:</span>
<span class="sd">    Baseline = 0.536</span>
<span class="sd">    LogReg = 0.457</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="n">classOneFracVal</span> <span class="o">=</span> <span class="n">OHEValidationData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">logLossValBase</span> <span class="o">=</span> <span class="n">OHEValidationData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">computeLogLoss</span><span class="p">(</span><span class="n">classOneFracTrain</span><span class="p">,</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">logLossValLR0</span> <span class="o">=</span> <span class="n">evaluateResults</span><span class="p">(</span><span class="n">model0</span><span class="p">,</span> <span class="n">OHEValidationData</span><span class="p">)</span>
<span class="k">print</span> <span class="p">(</span><span class="s">&#39;OHE Features Validation Logloss:</span><span class="se">\n\t</span><span class="s">Baseline = {0:.3f}</span><span class="se">\n\t</span><span class="s">LogReg = {1:.3f}&#39;</span>
       <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logLossValBase</span><span class="p">,</span> <span class="n">logLossValLR0</span><span class="p">))</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">output:</span>
<span class="sd">OHE Features Validation Logloss:</span>
<span class="sd">    Baseline = 0.528</span>
<span class="sd">    LogReg = 0.457</span>
<span class="sd">&#39;&#39;&#39;</span>
</code></pre></div>
<p>After feature hashing and training several models for different step-sizes and regularization constants,
the evaluation of the best model is shown below.</p>
<div class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">logLossTest</span> <span class="o">=</span> <span class="n">evaluateResults</span><span class="p">(</span><span class="n">bestModel</span><span class="p">,</span> <span class="n">hashTestData</span><span class="p">)</span>

<span class="c"># Log loss for the baseline model</span>
<span class="n">classOneFracTest</span> <span class="o">=</span> <span class="n">hashTestData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">logLossTestBaseline</span> <span class="o">=</span> <span class="n">hashTestData</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">point</span><span class="p">:</span> <span class="n">computeLogLoss</span><span class="p">(</span><span class="n">classOneFracTest</span><span class="p">,</span> <span class="n">point</span><span class="o">.</span><span class="n">label</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="k">print</span> <span class="p">(</span><span class="s">&#39;Hashed Features Test Log Loss:</span><span class="se">\n\t</span><span class="s">Baseline = {0:.3f}</span><span class="se">\n\t</span><span class="s">LogReg = {1:.3f}&#39;</span>
       <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">logLossTestBaseline</span><span class="p">,</span> <span class="n">logLossTest</span><span class="p">))</span>

<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">output:</span>
<span class="sd">Hashed Features Test Log Loss:</span>
<span class="sd">    Baseline = 0.537</span>
<span class="sd">    LogReg = 0.456</span>
<span class="sd">&#39;&#39;&#39;</span>
</code></pre></div>
</article>


<aside class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/03/28/nodejs-note-2/">
            Server-side Development Using NodeJS (2)
            <small><time datetime="2016-03-28T00:00:00-07:00">28 Mar 2016</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/03/27/nodejs-note-1/">
            Server-side Development Using NodeJS (1)
            <small><time datetime="2016-03-27T00:00:00-07:00">27 Mar 2016</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2015/11/04/flickfinder/">
            iOS Networking with Swift II: Flick Finder
            <small><time datetime="2015-11-04T00:00:00-08:00">04 Nov 2015</time></small>
          </a>
        </h3>
      </li>
    
  </ul>
</aside>


<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  
<!-- Add Disqus comments. -->
<div id="disqus_thread"></div>
<script type="text/javascript">
  /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
  var disqus_shortname = 'loganyc1934'; // required: replace example with your forum shortname
  var disqus_identifier = "/2015/10/14/logistic-regression-on-apache-spark/";

  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>



      <div class="footer">
        <p>
          &copy; 2016. All rights reserved.
        </p>
      </div>
    </div>

  </body>
</html>

<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>